{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More details:    \n",
    "https://github.com/LouYu2015/aws-fpga-top-tagging-notebooks/blob/master/Testing%20weights.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.python.platform import gfile\n",
    "def load_graph(sess, model_file_name, node_mapping={}):\n",
    "    with gfile.GFile(model_file_name, 'rb') as f:\n",
    "        graph_def = tf.GraphDef()\n",
    "        graph_def.ParseFromString(f.read())\n",
    "        tf.import_graph_def(graph_def, node_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    # Create a placeholder for the input\n",
    "    input_node = tf.placeholder(tf.float32, shape = [None, 224, 224, 3], name='Placeholder')\n",
    "    graph_def = load_graph(sess=sess,\n",
    "                           model_file_name='weights-quantized-224x224-fixval-best-final/constantgraph.pb',\n",
    "                           node_mapping={'Placeholder': input_node})\n",
    "# Get the node for output\n",
    "output_node = tf.get_default_graph().get_tensor_by_name(\"import/classifier/model_1/classifier_output/Softmax:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imported from utils \n",
    "# utils.py: https://github.com/nhanvtran/MachineLearningNotebooks/blob/nvt/bwcustomweights-validate/project-brainwave/utils.py\n",
    "\n",
    "def normalize_and_rgb(images): \n",
    "    import numpy as np\n",
    "    #normalize image to 0-255 per image.\n",
    "    image_sum = 1/np.sum(np.sum(images,axis=1),axis=-1)\n",
    "    given_axis = 0\n",
    "    # Create an array which would be used to reshape 1D array, b to have \n",
    "    # singleton dimensions except for the given axis where we would put -1 \n",
    "    # signifying to use the entire length of elements along that axis  \n",
    "    dim_array = np.ones((1,images.ndim),int).ravel()\n",
    "    dim_array[given_axis] = -1\n",
    "    # Reshape b with dim_array and perform elementwise multiplication with \n",
    "    # broadcasting along the singleton dimensions for the final output\n",
    "    image_sum_reshaped = image_sum.reshape(dim_array)\n",
    "    images = images*image_sum_reshaped*255\n",
    "\n",
    "    # make it rgb by duplicating 3 channels.\n",
    "    images = np.stack([images, images, images],axis=-1)\n",
    "    \n",
    "    return images\n",
    "\n",
    "def count_events(train_files):\n",
    "    import tables\n",
    "    n_events = 0\n",
    "    for train_file in train_files:\n",
    "        f = tables.open_file(train_file, 'r')\n",
    "        n_events += f.root.label.shape[0]\n",
    "        f.close()\n",
    "    return n_events\n",
    "\n",
    "\n",
    "\n",
    "def chunks(files, chunksize, max_q_size=4, shuffle=True): \n",
    "    \"\"\"Yield successive n-sized chunks from a and b.\"\"\" \n",
    "    import tables\n",
    "    import numpy as np\n",
    "    for train_file in files: \n",
    "        f = tables.open_file(train_file, 'r') \n",
    "        nrows = f.root.label.nrows\n",
    "        for istart in range(0,nrows,max_q_size*chunksize):  \n",
    "            a = np.array(f.root.img_pt[istart:istart+max_q_size*chunksize]) # Images \n",
    "            b = np.array(f.root.label[istart:istart+max_q_size*chunksize]) # Labels \n",
    "            if shuffle: \n",
    "                c = np.c_[a.reshape(len(a), -1), b.reshape(len(b), -1)] # shuffle within queue size\n",
    "                np.random.shuffle(c)\n",
    "                test_images = c[:, :a.size//len(a)].reshape(a.shape)\n",
    "                test_labels = c[:, a.size//len(a):].reshape(b.shape)\n",
    "            else:\n",
    "                test_images = a\n",
    "                test_labels = b\n",
    "            for jstart in range(0,len(test_labels),chunksize): \n",
    "                yield normalize_and_rgb(test_images[jstart:jstart+chunksize].copy()),test_labels[jstart:jstart+chunksize].copy(), len(test_labels[jstart:jstart+chunksize].copy())  \n",
    "        f.close()\n",
    "\n",
    "def test_model(preds, in_images, test_files, chunk_size=64, shuffle=True):\n",
    "    \"\"\"Test the model\"\"\"\n",
    "    import tensorflow as tf\n",
    "    from keras import backend as K\n",
    "    from keras.objectives import binary_crossentropy \n",
    "    import numpy as np\n",
    "    from keras.metrics import categorical_accuracy\n",
    "    from tqdm import tqdm\n",
    "    \n",
    "    in_labels = tf.placeholder(tf.float32, shape=(None, 2))\n",
    "    \n",
    "    cross_entropy = tf.reduce_mean(binary_crossentropy(in_labels, preds))\n",
    "    accuracy = tf.reduce_mean(categorical_accuracy(in_labels, preds))\n",
    "    auc = tf.metrics.auc(tf.cast(in_labels, tf.bool), preds)\n",
    "   \n",
    "    n_test_events = count_events(test_files)\n",
    "    chunk_num = int(n_test_events/chunk_size)+1\n",
    "    preds_all = []\n",
    "    label_all = []\n",
    "    \n",
    "    sess = tf.Session()\n",
    "    sess.run(tf.local_variables_initializer())\n",
    "    \n",
    "    avg_accuracy = 0\n",
    "    avg_auc = 0\n",
    "    avg_test_loss = 0\n",
    "    is_training = tf.get_default_graph().get_tensor_by_name('import/is_training:0')\n",
    "    n_current_events = 0\n",
    "    for img_chunk, label_chunk, real_chunk_size in chunks(test_files, chunk_size, shuffle=shuffle):\n",
    "        test_loss, accuracy_result, auc_result, preds_result = sess.run([cross_entropy, accuracy, auc, preds],\n",
    "                        feed_dict={in_images: img_chunk,\n",
    "                                   in_labels: label_chunk,\n",
    "                                   K.learning_phase(): 0,\n",
    "                                   is_training: False})\n",
    "        avg_test_loss += test_loss * real_chunk_size / n_test_events\n",
    "        avg_accuracy += accuracy_result * real_chunk_size / n_test_events\n",
    "        avg_auc += auc_result[0]  * real_chunk_size / n_test_events \n",
    "        preds_all.extend(preds_result)\n",
    "        label_all.extend(label_chunk)\n",
    "        n_current_events += real_chunk_size\n",
    "    \n",
    "        print(\"test_loss = \", \"{:.3f}\".format(avg_test_loss*n_test_events/n_current_events), end=\"\")\n",
    "        print(\"Test Accuracy:\", \"{:.3f}\".format(avg_accuracy*n_test_events/n_current_events), \", Area under ROC curve:\", \"{:.3f}\".format(avg_auc*n_test_events/n_current_events))\n",
    "    \n",
    "    return avg_test_loss, avg_accuracy, avg_auc, np.asarray(preds_all).reshape(n_test_events,2), np.asarray(label_all).reshape(n_test_events,2)32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_ft, accuracy_ft, auc_ft, preds_test_ft, test_labels_ft = test_model(output_node, input_node, ['test_file_0.h5'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os,sys\n",
    "\n",
    "if not os.path.exists(\"results/\"):\n",
    "    os.mkdir(\"results/\")\n",
    "results_dir = os.path.expanduser(\"results/\")\n",
    "#from utils import save_results\n",
    "def save_results(results_dir, prefix, accuracy, labels, preds, feats=None):\n",
    "    import numpy as np\n",
    "    \n",
    "    np.save(results_dir + \"/\" + prefix + \"_accuracy.npy\", accuracy)\n",
    "    np.save(results_dir + \"/\" + prefix + \"_labels.npy\", labels)\n",
    "    np.save(results_dir + \"/\" + prefix + \"_preds.npy\", preds)\n",
    "    if feats is not None:\n",
    "        np.save(results_dir + \"/\" + prefix + \"_feats.npy\", feats)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_results(results_dir, 'ft', accuracy_ft, test_labels_ft, preds_test_ft)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
